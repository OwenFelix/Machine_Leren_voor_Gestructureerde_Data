{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is created for the course _Machine Learning for Structured Data_ from the University of Amsterdam. By completing this lab you will gain basic knowledge about PyTorch which will help you spend less time during the assignments on debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### What is PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is an open-source deep learning framework designed to be flexible and modular, used both in research and production. It supports high-level functionalities such as tensor computation and GPU acceleration. PyTorch is developed and maintained by Meta's AI Research lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has certain advantages:\n",
    "- modular\n",
    "- dynamic computational graphs\n",
    "- automatic differentiation\n",
    "- GPU acceleration\n",
    "- popularity (both in research and production)\n",
    "\n",
    "By the end of this lab, you will able to experience most of its advantages and understand what they mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation and Setup for Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the assignments we will provide you with a _requirements.txt_ file contianing all the required packages and their versions for running the files.\n",
    "\n",
    "To install the packages in the Python environment associated with the currently active Python interpreter:\n",
    "\n",
    "```pip install -r requirements.txt ```\n",
    "\n",
    "If you are using [Anaconda](https://docs.anaconda.com/free/anaconda/install/) to manage your environments run in your terminal:\n",
    "\n",
    "```conda create --name mlsd --file requirements.txt```\n",
    "\n",
    "In this way, we ensure that everyone uses the same versions of packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the purpose of this lab, run the following cell to import eveything that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "# 2. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensor is a multi-dimensional array/matrix. A Tensor stores and manipulates the data in a more efficient way. The simples way to allocate memory for a tensor is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d tensor\n",
    "x = torch.empty(2, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D tensor\n",
    "x = torch.empty(3, 2, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4D tensor\n",
    "x = torch.empty(3, 2, 3, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the last tensor and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: print the shape of x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: print the first element in the tensor and its shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: print x[0][0], its transpose and the shape of the transpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the documentation and create a tensor containing just zeros and just ones of different shapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Indexing__ in PyTorch has one more dimension than the indexing in NumPy for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([\n",
    "                  [[1, 2], [3, 4]],\n",
    "                  [[5, 6], [7, 8]], \n",
    "                  [[9, 10], [11, 12]] \n",
    "                 ])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0, :]) # equivalent to x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 0, 0] # in words: loop over the first dimension and always select the element x[i][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the second elements of the first rows\n",
    "x[:, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get a list of all the second elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Flattening__ a tensor refers to converting a tensor with multiple dimensions into a one-dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(3,2,4)\n",
    "x_flattened = x.flatten()\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "print(f\"Shape: {x.shape}\")\n",
    "print(\"Flattened tensor:\")\n",
    "print(x_flattened)\n",
    "print(f\"Shape: {x_flattened.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Unsqueezing__ a tensor adds an additional dimension to a tensor at a specific position. More precisely, it increases the tensor's rank (number of dimensions) by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4])\n",
    "print(f\"Shape: {x.shape}\")\n",
    "x_unsqueezed_0 = x.unsqueeze(0)\n",
    "x_unsqueezed_1 = x.unsqueeze(-1) # equivalent to x.unsqueeze(1) because it is the last dimension of this tensor\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "print(\"Unsqueezed tensor on the first dimension:\")\n",
    "print(x_unsqueezed_0)\n",
    "print(\"Unsqueezed tensor on the last dimension:\")\n",
    "print(x_unsqueezed_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5,3,4)\n",
    "# TODO: check the documentation to reshape x into a (5,12) tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Element-wise operations__ on tensors range from addition, substraction, multiplication, division. Let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "y = torch.tensor([[5, 6], [7, 8]])\n",
    "summing_tensors = x + y\n",
    "substr_tensors = x - y\n",
    "mult_tensors = x * y # this is not matrix multiplication\n",
    "div_tensors = x / y\n",
    "print('x')\n",
    "print(x)\n",
    "print('y')\n",
    "print(y)\n",
    "print(\"Element-wise summing:\")\n",
    "print(summing_tensors)\n",
    "print(\"Element-wise substraction:\")\n",
    "print(substr_tensors)\n",
    "print(\"Element-wise multiplication:\")\n",
    "print(mult_tensors)\n",
    "print(\"Element-wise division:\")\n",
    "print(div_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Multiplying matrices__ is easy in PyTorch. All you have to do is match their dimensions! Okay maybe it is a bit complicated when you work in 3d, 4d.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(10, 32, 64)\n",
    "y = torch.empty(10, 64, 512)\n",
    "result = x @ y\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(10, 32, 64)\n",
    "y = torch.empty(512, 64)\n",
    "# TODO: match dimensions to multiply x and y and obtain a (10, 32, 512) tensor. Hint: transpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(64, 8, 256)\n",
    "y = torch.empty(512, 10)\n",
    "# TODO: match dimenstions to muliply x and y and obtain a (256, 10) tensor. Hint: 64*8=512.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has __built-in functions__ which are incredibly useful because they are optimized, simple, and dispose of numerical stability checks (e.g. for torch.log, torch.exp, torch.softmax). Such in-build functions include sum, mean, max, min and so on. By using these function, you take advantage of the computational power of PyTorch and allow to apply it per dimension of tensors without the need of unpacking any in between dimensions. Therefore, the computation is speeded up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([\n",
    "                  [[1, 2], [3, 4]],\n",
    "                  [[5, 6], [7, 8]], \n",
    "                  [[9, 10], [11, 12]] \n",
    "                 ])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"What is happening behind the scenes for x.sum(dim=0):\")\n",
    "print(x[:, 0, 0])\n",
    "print(x[:, 0, 0].sum())\n",
    "\n",
    "print(x[:, 0, 1])\n",
    "print(x[:, 0, 1].sum())\n",
    "\n",
    "print(x[:, 1, 0])\n",
    "print(x[:, 1, 0].sum())\n",
    "\n",
    "print(x[:, 1, 1])\n",
    "print(x[:, 1, 1].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.min(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "x = torch.randint(low=1, high=11, size=(3, 2, 4))\n",
    "# calculate the mean of all the last rows of each of the three (2,4) elements in the tensor\n",
    "last_rows = x[:, -1, :]\n",
    "mean_last_rows = torch.mean(last_rows, dim=1, dtype=float)\n",
    "print(x)\n",
    "print(mean_last_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "x = torch.randint(low=1, high=11, size=(3, 2, 5), dtype=torch.float64)\n",
    "# TODO: calculate the mean of all the first rows of each of the three (2,4) elements in the tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradients in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also has a differentiation feature. With a certain flag, PyTorch will remember the gradients. The backward() function requires PyTorch to calculate the gradients, which are then stored in the grad attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "y = 5*x*x + 1 # 5x^2 + 1\n",
    "y.backward()\n",
    "print(x.grad) # d(y)/d(x) = d(5x^2+1)/d(x) = 10x = 10 * [2.] = [20.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(start=-12, end=10, steps=100).requires_grad_()\n",
    "y = torch.sigmoid(x) # 1/(1+e^(-x))\n",
    "scalar_y = torch.sum(y)\n",
    "scalar_y.backward()\n",
    "gradients = x.grad\n",
    "\n",
    "# plot the function y and its first derivative function\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy(), label='sigmoid function')\n",
    "plt.plot(x.detach().numpy(), gradients.detach().numpy(), label=\"first derivative of sigmoid\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(start=-12, end=10, steps=100).requires_grad_()\n",
    "# TODO: add below another function to check how its first derivative looks like\n",
    "y = ... # suggestion: inspo from documentation (i.e. torch.relu(x), nn.ELU()(x), nn.PReLU()(x))\n",
    "scalar_y = torch.sum(y)\n",
    "scalar_y.backward()\n",
    "gradients = x.grad\n",
    "\n",
    "# plot the function y and its first derivative function\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy(), label='your function')\n",
    "plt.plot(x.detach().numpy(), gradients.detach().numpy(), label='its first derivative')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modularity in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a very modular framework. This makes it eazy to read and to interpret, eazy to connect pieces of networks together. The code below is a quick demo on how a class works, the way to access variables and to declare them. It is a simple class called MyMeal which takes a list of string ingredients as inputs and encodes them in natural numbers. What you can do with a meal is prepare it, meaning that a combination of the ingredients will be randomly selected and set as the meal. The second thing you can do with a meal is eat it, but first it checks if the meal was prepared in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is strange to use PyTorch for this task, but it is a good reminder on how classes work. Notice how different torch functions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMeal:\n",
    "    def __init__(self, ingredients) -> None:\n",
    "        self.ingredients = ingredients\n",
    "        self.ing_encodings = torch.tensor(list(range(len(ingredients))))\n",
    "        self.meal = torch.zeros(len(ingredients))\n",
    "    def prepare(self, number_of_ingredients = 3):\n",
    "        combinations = torch.combinations(self.ing_encodings, r=number_of_ingredients)\n",
    "        rand_idx = torch.randint(high=combinations.size(0), size=(1,)).item()\n",
    "        self.meal = combinations[rand_idx]\n",
    "    def eat(self):\n",
    "        if not torch.all(self.meal == 0).item():\n",
    "            selected_ingredients = [self.ingredients[idx] for idx in self.meal]\n",
    "            self.meal = torch.zeros(len(self.ingredients))\n",
    "            return selected_ingredients\n",
    "        else:\n",
    "            return 'Prepare it first!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = ['Flour', 'Sugar', 'Eggs', 'Butter']\n",
    "meal = MyMeal(ingredients)\n",
    "meal.eat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal.prepare()\n",
    "meal.eat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on how classes work in Python, check [__Practical Programming__](https://pragprog.com/titles/gwpy3/practical-programming-third-edition/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nueral Networks are eveywhere in artificial intelligence. The main advantage of a neural network is that it can learn to approximate an unknown function able to separate some data. The network adjusts its parameters (weights and biases) during the training phase to minimize the difference between the network's predicted output and the true output of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting together everything from the basics of PyTorch, we can now use it to train a neural network for a classification task! In this section we will:\n",
    "- generate our own training and validation data\n",
    "- visualize the data\n",
    "- define our own model architecture\n",
    "- train the our model\n",
    "- validate the model\n",
    "- visualize what has been learned by the network\n",
    "- save the model and load it for more predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_circular_data(radius, noise_scale, num_samples):\n",
    "    angles = np.random.uniform(0, 2 * np.pi, num_samples)\n",
    "    x = radius * np.cos(angles)\n",
    "    y = radius * np.sin(angles)\n",
    "    noise = np.random.normal(scale=noise_scale, size=(num_samples, 2))\n",
    "    x += noise[:, 0]\n",
    "    y += noise[:, 1]\n",
    "    return np.column_stack((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "samples_class1 = sample_circular_data(radius=5, noise_scale=1.5, num_samples=num_samples)\n",
    "samples_class2 = sample_circular_data(radius=10, noise_scale=1.5, num_samples=num_samples)\n",
    "\n",
    "# Create dataset with classes\n",
    "data = np.concatenate((samples_class1, samples_class2), axis=0)\n",
    "labels = np.concatenate((np.zeros(num_samples), np.ones(num_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to tensors\n",
    "train_x = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_labels, dtype=torch.float32).view(-1,1)\n",
    "val_x = torch.tensor(val_data, dtype=torch.float32)\n",
    "val_y = val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the data is not linearlly separable. A linear classifier is not able to solve this clssification task without a poor performance. Here is where Neural Networks come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(samples_class1[:, 0], samples_class1[:, 1], label='Class 1')\n",
    "plt.scatter(samples_class2[:, 0], samples_class2[:, 1], label='Class 2')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Data Visualization')\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to learn a model to separate the data we visualized above into two classes. Therefore, this is a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our own architecture for the Neural Network. It is a simple neural network with:\n",
    "- three linear layers\n",
    "    - input size is 2 (since each input has an x-value and a y-value as features)\n",
    "    - output size is 1 since this is a binary classification and we optimize for an element belonging to a class or not\n",
    "    - 128, 64 represent the number of hidden units\n",
    "    - a dropout layer dropping 30% of the nodes during training\n",
    "- ReLU as the activation function\n",
    "- Sigmoid is used for the final layer because it is a binary classification task\n",
    "(note that the Sigmoid function produces proboabilities of one object being in a class)\n",
    "\n",
    "Feel free to change the number of Linear layers or hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the __forward__ function, each layer is passed thorugh the activation funciton, and at the end through the Sigmoid function to produce the probability of the input being in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the network\n",
    "model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train the model. We will use a Binary Cross Entropy Loss (i.e. nn.BCELoss()) and a Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important:__ set the model in training mode (i.e. model.train()) when training and in evaluation mode (i.e. model.eval()) when validating or testing the model. Why? For example, Dropout layers are only needed during training. Therefore, setting the flag right will give you the true performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "num_epochs = 2000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # TODO: set the model in training mode\n",
    "    \n",
    "\n",
    "    # Step 1: zero out the gradients of the parameters that are being optimized\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Step 2: forward pass\n",
    "    outputs = model(train_x)\n",
    "    loss = loss_fn(outputs, train_y)\n",
    "\n",
    "    # Step 3: backward pass and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # TODO: set the model in eval mode\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(val_x)\n",
    "        val_predictions = (val_outputs >= 0.5).squeeze().numpy()\n",
    "    \n",
    "    val_accuracy = (val_predictions == val_y).mean()\n",
    "\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "grid_tensor = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "Z = model(grid_tensor).detach().numpy().reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(samples_class1[:, 0], samples_class1[:, 1], label='Class 1')\n",
    "plt.scatter(samples_class2[:, 0], samples_class2[:, 1], label='Class 2')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Decision Boundary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Save and Load a Model (reusing the pretrained weigths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = SimpleNet()\n",
    "loaded_model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the weights for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generate some play test data in the same way as above and check how your loaded model performs on it\n",
    "\n",
    "num_samples = 500\n",
    "\n",
    "new_samples_class1 = ...\n",
    "new_samples_class2 = ...\n",
    "\n",
    "# Create dataset with classes\n",
    "new_data = np.concatenate((new_samples_class1, new_samples_class2), axis=0)\n",
    "data_y = np.concatenate((np.zeros(num_samples), np.ones(num_samples)))\n",
    "\n",
    "data_x = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    outputs = model(data_x)\n",
    "    predictions = (outputs >= 0.5).squeeze().numpy()\n",
    "\n",
    "accuracy = (predictions == data_y).mean()\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. More resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Official PyTorch Tutorial](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)\n",
    "- [Standford Notebook on PyTorch](https://web.stanford.edu/class/cs224n/materials/CS224N_PyTorch_Tutorial.html)\n",
    "- [UvA PyTorch Notebook](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html)\n",
    "- [Kaggle notebook on how gradinet descent works in neural networks](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
